- 문법과 어휘 형태소를 다른 방식으로 토큰화함으로써 모델에 형태소 유형 정보를 제공하고, 이것이 모델의 성능에 어떠한 영향을 미치는지를 탐구함

- 논문
    * 전태희(2022). "한국어 텍스트의 토큰화 방법에 관한 언어학적 연구: fastText 단어 임베딩을 이용하여", 언어사실과 관점(연세대학교 언어정보연구원) 55, 309-354.
- 초록
    * This paper aims to provide linguistic understanding of how tokenization methods affect the performance of deep-learning-based NLP (Natural Language Processing) models. To this end, we train fastText word embedding models using different tokenization methods. The methods are categorized by their tokenization unit, eojeol or morpheme, and subword level, syllable (character) or grapheme (subcharacter). Additionally, we apply selective grapheme-decomposition according to the type of a morpheme, lexical or grammatical, for the morpheme tokenization methods to provide linguistic information to models. Using these embedding models, we perform a similarity test, an analogy test, and sentiment analysis. The results show that the selective grapheme-decomposition methods can improve the performance of a model by preventing duplicated learning of some morphemes whose morpheme types are different from each other but subwords are similar or keeping meaningful syllables of lexical morphemes. It is also suggested that grapheme-decomposition after morpheme tokenization would be useful for resolving not only Out-of-Vocabulary problems but also errors of a morphological analyzer.